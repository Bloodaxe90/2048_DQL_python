{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T23:42:56.617099Z",
     "start_time": "2024-12-19T23:42:55.762921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "from torch import nn\n",
    "from src.utils.inference import *\n",
    "from src.game.tweny48 import *\n",
    "from src.game.dynamics import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:42:57.327513Z",
     "start_time": "2024-12-19T23:42:57.290700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_prediction(model: nn.Module ,state: np.ndarray) -> torch.tensor:\n",
    "    model.eval()\n",
    "    state = encode_state_batch([state])\n",
    "    device = get_device()\n",
    "    with torch.inference_mode():\n",
    "        state = state.to(get_device())\n",
    "        pred = model(state).to(device)\n",
    "\n",
    "    return pred"
   ],
   "id": "bee1bb423b71a",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:42:57.899619Z",
     "start_time": "2024-12-19T23:42:57.772668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        d = out_channels // 4\n",
    "        self.conv1 = nn.Conv2d(in_channels, d, 1, padding='same')\n",
    "        self.conv2 = nn.Conv2d(in_channels, d, 2, padding='same')\n",
    "        self.conv3 = nn.Conv2d(in_channels, d, 3, padding='same')\n",
    "        self.conv4 = nn.Conv2d(in_channels, d, 4, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.conv1(x)\n",
    "        output2 = self.conv2(x)\n",
    "        output3 = self.conv3(x)\n",
    "        output4 = self.conv4(x)\n",
    "        return torch.cat((output1, output2, output3, output4), dim=1)\n",
    "\n",
    "class DQModelConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_neurons: int, hidden_neurons: tuple, output_neurons: int, state_size: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.input_block = nn.Sequential(\n",
    "            ConvBlock(in_channels= input_neurons,\n",
    "                      out_channels= hidden_neurons[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.hidden_blocks = nn.ModuleList()\n",
    "        for i in range(1, len(hidden_neurons)):\n",
    "            self.hidden_blocks.append(nn.Sequential(\n",
    "               ConvBlock(in_channels= hidden_neurons[i-1],\n",
    "                      out_channels= hidden_neurons[i]),\n",
    "               nn.ReLU(),\n",
    "            ))\n",
    "\n",
    "        flattened_in_features = hidden_neurons[-1]*(state_size**2)\n",
    "        flatten_out_features = hidden_neurons[-1] //2\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=flattened_in_features,\n",
    "                      out_features=flatten_out_features),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=flatten_out_features,\n",
    "                      out_features=output_neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_block(x)\n",
    "        for hidden_block in self.hidden_blocks:\n",
    "            x = hidden_block(x)\n",
    "        return self.output_block(x)"
   ],
   "id": "ebe30b1f931815f0",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:42:58.135715Z",
     "start_time": "2024-12-19T23:42:58.095363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_neurons: int, hidden_neurons: tuple, output_neurons: int, state_size: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.input_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= input_neurons,\n",
    "                      out_channels= hidden_neurons[0],\n",
    "                      kernel_size=3,\n",
    "                      padding=\"same\"),\n",
    "            nn.BatchNorm2d(hidden_neurons[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.hidden_blocks = nn.ModuleList()\n",
    "        for i in range(1, len(hidden_neurons)):\n",
    "            self.hidden_blocks.append(nn.Sequential(\n",
    "               nn.Conv2d(in_channels= hidden_neurons[i-1],\n",
    "                      out_channels= hidden_neurons[i],\n",
    "                      kernel_size=2,\n",
    "                      padding=\"same\"),\n",
    "               nn.BatchNorm2d(hidden_neurons[i]),\n",
    "               nn.ReLU(),\n",
    "            ))\n",
    "\n",
    "        flattened_in_features = hidden_neurons[-1]*(state_size**2)\n",
    "        flatten_out_features = flattened_in_features // 2\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=flattened_in_features,\n",
    "                      out_features=flatten_out_features),\n",
    "            nn.BatchNorm1d(flatten_out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=flatten_out_features,\n",
    "                      out_features=output_neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_block(x)\n",
    "        for hidden_block in self.hidden_blocks:\n",
    "            x = hidden_block(x)\n",
    "        return self.output_block(x)\n"
   ],
   "id": "f745afa9d63e57a8",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:42:58.566478Z",
     "start_time": "2024-12-19T23:42:58.540069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer(deque):\n",
    "\n",
    "    def __init__(self, capacity: int = 6000):\n",
    "        super().__init__(maxlen=capacity)\n",
    "        self.capacity: int = capacity\n",
    "\n",
    "    def append(self, transition):\n",
    "        super().append(transition)\n",
    "\n",
    "    def full(self) -> bool:\n",
    "        return len(self) >= self.maxlen\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        return random.sample(self, batch_size)\n",
    "\n"
   ],
   "id": "7a3ea84fd3e49c35",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:42:59.035669Z",
     "start_time": "2024-12-19T23:42:59.014175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_state_batch(states: list[np.ndarray]):\n",
    "\n",
    "    states_flat = []\n",
    "    for state in states:\n",
    "        state_log = np.where(state == 0, 0, np.log2(state)).astype(np.int64)\n",
    "        states_flat.append(state_log)\n",
    "\n",
    "    states_tensor = torch.Tensor(np.array(states_flat)).long().to(get_device())\n",
    "\n",
    "    states_one_hot = torch.nn.functional.one_hot(states_tensor, num_classes=12).float().to(get_device())\n",
    "\n",
    "    states_one_hot = states_one_hot.permute(0, 3, 1, 2)\n",
    "\n",
    "    return states_one_hot"
   ],
   "id": "c8af033f85621bd7",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:42:59.578497Z",
     "start_time": "2024-12-19T23:42:59.547018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_reward1(current_state, next_state, actions, win_val):\n",
    "    reward = 0\n",
    "    result = check_terminal(next_state, actions, win_val)\n",
    "    if result ==\"W\":\n",
    "        reward += 10\n",
    "    elif result == \"L\":\n",
    "        reward -= 10\n",
    "    elif np.array_equal(current_state, next_state):\n",
    "        reward -= 5\n",
    "\n",
    "    return reward"
   ],
   "id": "977034aa96d133e7",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:43:00.412969Z",
     "start_time": "2024-12-19T23:43:00.104549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQL(Twenty48):\n",
    "\n",
    "    def __init__(self, win_val = 256, max_epsilon: float = 0.9, min_epsilon:float = 0.01, alpha: float = 0.00005):\n",
    "        super().__init__()\n",
    "        self.main_network: DQModelConvBlock = self.create_network().to(get_device())\n",
    "        self.target_network: DQModelConvBlock = self.create_network().to(get_device())\n",
    "        self.replay_buffer: ReplayBuffer = ReplayBuffer(6000)\n",
    "\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.win_val = win_val\n",
    "        self.epsilon = max_epsilon\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.main_network.parameters(), lr=alpha)\n",
    "\n",
    "    def train(self, episodes, batch_size: int= 32):\n",
    "        results = []\n",
    "        total_scores = []\n",
    "        steps = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for episode in range(1, episodes):\n",
    "            while (result := check_terminal(self.environment, self.ACTIONS, self.win_val)) == \"\":\n",
    "                self.interact()\n",
    "                steps += 1\n",
    "            self.decay_epsilon(episode, episodes)\n",
    "\n",
    "            results.append(result)\n",
    "            if len(self.replay_buffer) > batch_size:\n",
    "                for _ in range(100):\n",
    "                    total_loss += self.update_main_network(self.loss_fn, self.optimizer, batch_size)\n",
    "\n",
    "            if episode % 20 == 0:\n",
    "                self.update_target_network()\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                win_rate = sum([1 for r in results if r == \"W\"]) / 10\n",
    "                print(f\"Episode: {episode} | Result: {win_rate} | Highest_Val: {np.max(self.environment)}| avg_steps: {steps/episode} | epsilon: {self.epsilon} | loss: {total_loss/(100*10)}\")\n",
    "                results = []\n",
    "                total_loss = 0\n",
    "\n",
    "            total_scores.append(self.total)\n",
    "            if episode > 50:\n",
    "                average = sum(total_scores[-50:]) / 50\n",
    "                print(f\"50 episode running average: {average}\")\n",
    "            self.reset()\n",
    "            self.total = 0\n",
    "\n",
    "\n",
    "    def interact(self):\n",
    "        current_state = self.copy()\n",
    "        action = self.get_action()\n",
    "        moved, merge_values = game_step(self.environment, action)\n",
    "        reward = get_reward1(current_state, self.environment,self.ACTIONS, self.win_val)\n",
    "        next_state = self.copy()\n",
    "        done = 1 if check_terminal(self.environment, self.ACTIONS) != \"\" else 0\n",
    "\n",
    "        self.total += sum(merge_values)\n",
    "\n",
    "        if check_terminal(self.environment, self.ACTIONS, self.win_val) != \"\" or len(self.replay_buffer) == 0 or (moved or merge_values):\n",
    "            self.replay_buffer.append((current_state, action, reward, next_state, done))\n",
    "\n",
    "    def decay_epsilon(self, episode, max_episodes, power: float= 1.2):\n",
    "        fraction = episode/max_episodes\n",
    "        self.epsilon = (self.max_epsilon - self.min_epsilon) * ((1 - fraction) ** power) + self.min_epsilon\n",
    "\n",
    "    def decay_epsilon1(self, episode, max_episodes, decay = 0.9999):\n",
    "        self.epsilon = max(self.min_epsilon, self.max_epsilon * (decay ** episode))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "\n",
    "    def update_main_network(self, loss_fn, optimizer, batch_size: int, gamma: float= 0.9):\n",
    "        device = get_device()\n",
    "\n",
    "        batch = list(self.replay_buffer.sample(batch_size))\n",
    "        states = encode_state_batch([transition[0] for transition in batch]).to(get_device())\n",
    "        actions = [transition[1] for transition in batch]\n",
    "        rewards = torch.Tensor([transition[2] for transition in batch]).to(get_device())\n",
    "        next_states = encode_state_batch([transition[3] for transition in batch]).to(get_device())\n",
    "        dones = torch.Tensor([transition[4] for transition in batch]).to(get_device())\n",
    "\n",
    "        self.main_network.train()\n",
    "        self.target_network.eval()\n",
    "\n",
    "        main_y = self.main_network(states).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            target_y = self.target_network(next_states).to(device)\n",
    "\n",
    "        action_indices = [self.ACTIONS.index(action) for action in actions]\n",
    "        current_q_values = main_y[range(len(actions)), action_indices]\n",
    "\n",
    "        max_next_q_values = torch.amax(target_y, dim=1)\n",
    "\n",
    "        target_q_values = rewards + ((1 - dones) * (gamma * max_next_q_values))\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(current_q_values, target_q_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def get_action(self) -> str:\n",
    "        if random.random() >= self.epsilon:\n",
    "            return self.get_best_action()\n",
    "        else:\n",
    "            return self.get_random_action()\n",
    "\n",
    "    def get_random_action(self) -> str:\n",
    "        invalid_actions = get_invalid_actions(self.environment, self.ACTIONS)\n",
    "        valid_actions = [action for action in self.ACTIONS if action not in invalid_actions]\n",
    "        return random.choice(self.ACTIONS)\n",
    "\n",
    "    def get_best_action(self) -> str:\n",
    "        invalid_actions  = get_invalid_actions(self.environment, self.ACTIONS)\n",
    "        invalid_indices = [i for i, action in enumerate(self.ACTIONS) if action in invalid_actions]\n",
    "\n",
    "        prediction = make_prediction(self.main_network, self.environment).squeeze()\n",
    "        masked_q_values = prediction.clone()\n",
    "        masked_q_values[invalid_indices] = -float('inf')\n",
    "        return self.ACTIONS[torch.argmax(prediction).item()]\n",
    "\n",
    "    def create_network(self, hidden_neurons: tuple = (64, 64, 64, 64), ):\n",
    "        return DQModelConvBlock(input_neurons=12,\n",
    "                       hidden_neurons=hidden_neurons,\n",
    "                       output_neurons=len(self.ACTIONS),\n",
    "                       state_size=len(self))\n",
    "\n",
    "    def update_target_network_advanced(self, tau: float = 0.001):\n",
    "        for target_param, main_param in zip(self.target_network.parameters(), self.main_network.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau * main_param.data + (1.0 - tau) * target_param.data\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2aee558d86a5f7ac",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:43:06.651186Z",
     "start_time": "2024-12-19T23:43:01.265829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dql = DQL()\n",
    "dql.train(200)"
   ],
   "id": "254ab713dfea36ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/_7nzg4ln01d8js37_knftkp40000gp/T/ipykernel_25904/1248462863.py:5: RuntimeWarning: divide by zero encountered in log2\n",
      "  state_log = np.where(state == 0, 0, np.log2(state)).astype(np.int64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[140], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m dql \u001B[38;5;241m=\u001B[39m DQL()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[139], line 32\u001B[0m, in \u001B[0;36mDQL.train\u001B[0;34m(self, episodes, batch_size)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_buffer) \u001B[38;5;241m>\u001B[39m batch_size:\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[0;32m---> 32\u001B[0m         total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_main_network\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m episode \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m20\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_target_network()\n",
      "Cell \u001B[0;32mIn[139], line 87\u001B[0m, in \u001B[0;36mDQL.update_main_network\u001B[0;34m(self, loss_fn, optimizer, batch_size, gamma)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmain_network\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_network\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m---> 87\u001B[0m main_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmain_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m     90\u001B[0m     target_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_network(next_states)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[134], line 50\u001B[0m, in \u001B[0;36mDQModelConvBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     48\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_block(x)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hidden_block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_blocks:\n\u001B[0;32m---> 50\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mhidden_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_block(x)\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[134], line 11\u001B[0m, in \u001B[0;36mConvBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 11\u001B[0m     output1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     output2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)\n\u001B[1;32m     13\u001B[0m     output3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(x)\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/2048/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:46:04.290354Z",
     "start_time": "2024-12-19T23:46:04.245616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trail_random_ai(ai, episodes: int) -> dict:\n",
    "    wins = 0\n",
    "    dic = {}\n",
    "    print(\"sum\")\n",
    "    for i in range(1, episodes):\n",
    "        while (result := check_terminal(ai.environment, ai.ACTIONS, ai.win_val)) == \"\":\n",
    "            action = ai.get_random_action()\n",
    "            moved, merge = game_step(ai.environment, action)\n",
    "            if not moved and not merge:\n",
    "                action = ai.get_random_action()\n",
    "                moved, merge = game_step(ai.environment, action)\n",
    "\n",
    "        max_val = np.max(ai.environment)\n",
    "        ai.reset()\n",
    "        wins += 1 if result == \"W\" else 0\n",
    "        if i % (episodes / 10) == 0:\n",
    "            print(i)\n",
    "        dic[max_val] = dic.get(max_val, 0) + 1\n",
    "    return dic"
   ],
   "id": "85af81524f263517",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:46:04.774653Z",
     "start_time": "2024-12-19T23:46:04.745417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_graph(results:dict):\n",
    "    tile_values = [float(x) for x in results.keys()]\n",
    "    occurrences = list(results.values())\n",
    "    # Sort data for better visualization\n",
    "    sorted_data = sorted(zip(tile_values, occurrences), key=lambda x: x[0])\n",
    "    sorted_tiles, sorted_occurrences = zip(*sorted_data)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(sorted_tiles, sorted_occurrences, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(\"Tile Value\", fontsize=12)\n",
    "    plt.ylabel(\"Occurrences\", fontsize=12)\n",
    "    plt.title(\"Tile Occurrences in 2048\", fontsize=14)\n",
    "    plt.xticks(sorted_tiles)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ],
   "id": "e72e0ad5387639cd",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:47:01.136339Z",
     "start_time": "2024-12-19T23:46:12.176980Z"
    }
   },
   "cell_type": "code",
   "source": "plot_graph(trail_random_ai(DQL(), 1000))",
   "id": "35efb3262d30fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAHbCAYAAADLf1JFAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAS59JREFUeJzt3XlclWX+//H34bCIyCIguCBuLWq5a0LuZpJj5jrWpKVlWY5aacPXatTSnNSaKZu+tpiO1pRtlpmOG7lWaoliLrlUouSKiIKKcpBz/f7ox/l6ZFEOKHDP6/l4nMd0rus69/l8zmnozc117mMzxhgBAAAAFuRV1gUAAAAA1wphFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhF4DHOnfuLJvN5ja2du1a2Ww2vfDCC2VTFCzthRdekM1m09q1a8u6FAAVBGEXgCTJZrMV61bWLly4oNdff10dOnRQWFiY/Pz8FBUVpYEDB2r16tVlXR4qqG+//VZPP/20WrVqpbCwMFWqVEkNGzbUuHHjdPr06UIft3nzZv3hD39QSEiIAgICFBMTo08//fSqnnPjxo2y2+2y2WyaNm1agWtOnz6tiRMnqmnTpgoMDFR4eLjatGmj//3f/9WFCxc8aRX4r+Fd1gUAKB+ef/75fGMzZsxQRkZGgXOS9P777ysrK+tal5bPL7/8op49e2rfvn2qX7++Bg4cqJCQEO3fv1//+c9/9Nlnn2n48OGaOXOmvL35MWclo0aN0n333afo6OhrcvwBAwYoLS1N7du314MPPug6i/zyyy9rwYIF2rBhgyIjI90es2bNGsXFxalSpUq67777FBgYqM8//1z33nuvfvvtNz399NOFPl9WVpaGDBkif39/nTt3rsA1p0+fVqtWrbR//361b99ejz32mLKzs7Vs2TKNHj1aCxcuVEJCgry8OH8FFMgAQCHq1KljivtjYs2aNUaSef75569JTadPnzYNGjQwksyECRPMxYsX3eYPHz5s2rRpYySZ+Pj4a1IDrGvatGnm8OHDbmNOp9OMGDHCSDJ//vOf3eZycnJMgwYNjJ+fn0lKSnKNnz592tx0003G19fXHDhwoNDnGzVqlAkODjZTpkwxkszUqVPzrZk+fbqRZJ566im38ezsbNO6dWsjyaxbt86DboH/DvwaCMBjBe3ZLUpqaqrGjBmjG264QX5+fgoPD1f//v21c+fOqz7GK6+8ol9//VWDBg3S5MmTZbfb3eZr1qypxYsXKzQ0VP/4xz/0yy+/5DvG+vXr1adPH0VGRsrPz0+1a9dWv3799O2337qtM8Zo7ty56tChg0JCQlS5cmXdeOONeuyxx5SSkuJaV7duXdWtW7fAegt6jS7ddzpv3jy1bNlSlStXVufOnd0ec+HCBY0fP14NGjSQj4+P2z7o5ORkPfLII4qOjpafn59q1KihoUOH6uDBg/lqsNls6ty5s44fP64hQ4YoPDxc/v7+iomJKXTv65kzZzRp0iQ1bdpUlStXVnBwsFq0aKEJEyYoJyfHbW1xatm6dasGDBjgWlutWjW1adNGf/vb3wqs43IF7dk9cOCAbDabhg4dql9++UV9+/ZV1apVFRAQoG7duunHH3+8qmNL0rhx41SzZk23MZvNpgkTJkiS1q1b5za3evVq/frrr7r//vvVvHlz13hwcLCee+45ORwOvffeewU+15o1azRz5ky9+uqrqlWrVqE17d+/X5L0hz/8wW3c19dX3bt3lySdOHHi6hoE/gvx9z0A18Wvv/6qzp0769ChQ+revbv69Omj1NRUff7551qxYoVWrVqltm3bXvE4c+fOlSRX+ChIZGSkHn30UU2fPl3z5s3TlClTXHOvv/66xowZI39/f/Xt21fR0dE6fPiwvv32Wy1YsEDt27eXJDmdTt17771asGCBatWqpT/96U8KCgrSgQMH9Omnn6pHjx4l/lP6K6+8ojVr1qh3797q3r17vuDev39//fjjj7rrrrsUEhKievXqSZK+//57xcXF6dy5c7r77rt144036sCBA/rwww+1bNkybdy4UfXr13c71unTp9W+fXsFBwfrgQceUGpqqj755BPFxcVpy5YtuvXWW11rU1NT1alTJ+3Zs0fNmzfXiBEj5HQ6tWfPHk2fPl1PP/20QkJCil3Ltm3bdPvtt8tut6t3796qU6eOTp8+rZ9++kmzZs3SX//61xK9ngcOHFBMTIxuueUWPfzww/r111+1aNEidenSRbt37863/aA4fHx8JCnftpi80J0XOi8VFxcnKX9Aln7/ZeKhhx5S9+7d9fDDD2vevHmFPnfee7N06VLdeeedrnGHw6GEhAT5+/srNja2WP0A/1XK+tQygPLrStsYOnXqlG++sG0Mt99+u7Hb7Wb58uVu43v37jWBgYGmSZMmV6znwIEDRpKpVavWFdeuXLnSSDJdu3Z1jW3bts14eXmZmjVrmuTkZLf1TqfT7c/Xb7zxhpFk7rjjDpOVleW2Nisry5w8edJ1v06dOqZOnToF1lHQa/T8888bSSYgIMBs37690Mc0b97c7XmMMcbhcJi6deuawMBAs3XrVre5b775xtjtdnP33Xe7jUty/Qk+NzfXNT579mwjyTz22GNu6/v3728kmeeeey5fbceOHTM5OTke1TJ27FgjyXz55Zf5jpuWlpZvrCB5r92aNWtcY8nJya4ep02b5rZ+/PjxhW4PKI68rQSXb40ZMGCAkWQSExMLfFyVKlVM7dq1840PGzbMBAUFmZSUFGOMMXPnzi20zqysLBMTE2MkmQ4dOpi//OUvZvTo0aZBgwYmMjLSfPXVVyXqDbA6wi6AQpVW2N26dauRZB5++OECj5MXgnbs2FFkPZs2bTKSTExMzBVr3717t5FkGjVq5BrL23f5r3/964qPb9SokbHb7Wbfvn1XXOtp2B0zZkyRj1m0aFG+uS+++MJIMpMnTy7wsf369TNeXl4mIyPDNZYXrM+cOeO2Nicnx3h7e5uWLVu6xo4ePWpsNptp0KCBcTgcBT6Hp7Xkvc8rVqwo8rhFKSrs1qtXzy3MXzrXr18/j58zKSnJVK5c2URERJgTJ064zd15551Gkvn5558LfGzNmjVNUFCQ29jSpUuNJPPOO++4xooKu8b8HniHDBniCvWSjN1uN0899dRV/6IA/LdiGwOAa27Tpk2SpOPHjxd4/d09e/a4/vfSP6eXth9++EFSwX9yvtTZs2e1e/du3XDDDbrxxhuvWT233XZbsefzXsu9e/cW+FoeO3ZMTqdT+/btU+vWrV3jN910k6pUqeK21tvbW5GRkW6X1EpMTJQxRl26dHH96b4wxa1l4MCBmjFjhvr27at7771Xd955pzp27FjkftXiaN68eb4rEkRFRUlSkZcNK8r+/fvVs2dP5ebm6uOPP1Z4eHiJajx16pQeeeQR3XHHHRo+fPhVPebEiRPq3bu3Tpw4oaVLl6pdu3bKysrSokWL9PTTT2vJkiXasmWLgoKCSlQbYFWEXQDXXHp6uiTpP//5j/7zn/8Uuq6wSy/lqV69uiTpt99+u+Jz5q2pUaOGaywjI0M2m81trCAZGRmSVGohrDBX2kNa0Hzea/nhhx8W+djLX8vCgpC3t7dyc3Nd94vTe3Fradu2rdauXauXXnpJ8+fPd+2/btOmjaZPn64uXbpc8TmLUlCPeXtsL+3xaiUnJ6tLly5KS0vT559/XmB9wcHBkv7vdbtcZmamqlat6ro/duxYZWRkaPbs2Vddx5gxY7Rx40b9+OOPatq0qaTfe33sscd04cIFPfXUU3rjjTdKvOcZsCquxgDgmssLIW+88YbM79unCrwNGTKkyOPUqVNHNWvW1OHDh7V3794i165atUqS3D64ExISImOMjh49WuRj8wLM4cOHr9ibJHl5eenixYsFzhUWgiRd8UoWBc3nvZaLFy8u8rXs1KnTVdV+ubwPnl1N757U0qFDBy1btkynTp3SmjVrNHbsWO3YsUM9e/Z0XXWgPNi/f786d+6so0eP6tNPP9Xdd99d4Lq8M/8///xzvrljx47p7Nmzbn8dSEpK0rlz51SvXj23L2l56KGHJEnPPvusbDabnnrqKddjli1bptDQUFfQvVReAE9KSvK4V8DqCLsArrm8qyxs3LixxMcaOnSoJBV5qarU1FTNnj1bXl5ervXS/20LWLlyZZHPUaVKFTVu3FjJyckFhpjLVa1aVampqfkC77lz567q8cVRmq9lQVq3bi0vLy+tWbMm3yXGSrMWf39/de7cWf/4xz/03HPP6fz580pISPCo5tK2f/9+denSRUePHtUnn3yi3r17F7o2L8gX9O/UihUr3NZIUr9+/TRs2LB8t44dO0r6/Sz3sGHD3H5JczgcyszMlMPhyPcceZcc8/Pz86BT4L/EddwfDKCCKc2rMbRt29bYbDbz8ccf5ztObm6uWbt27VXVdOrUKVOvXj0jyUyaNCnfl0ocPXrU9cn1yz85v337dmO3203NmjXzXej/8qsxzJw500gy3bp1y3c1hvPnz7tdJeGxxx4zksy8efPcjjd69GjXh4kuVdCHrC5V0Oua58KFCyY6OtpUqlSpwC8ScDgc5ptvvnEbk2Q6depU4PEK+nDdH//4RyPJ/PWvf823/vjx466rMRS3lg0bNpjz58/nWzdy5Mh8r19hivqA2pAhQwp8TFH9X27//v0mOjraeHt7m88///yK63Nyckz9+vWL/FKJy6/8UZCiPqAWFxdnJJnx48e7jZ8/f9507tzZSDLvvvvuFZ8D+G/Fnl0A18VHH32kLl266L777tOMGTPUsmVL+fv7KyUlRRs3btSJEyd04cKFKx4nJCREy5cvV8+ePfX888/r/fffV1xcnIKDg11fF3z27Fk9+uijeumll9we26RJE82YMUNPPPGEbrnlFvXp00d16tTRsWPHtH79evXs2VMzZsyQJI0YMULr1q3Tp59+qhtvvFH33HOPgoKClJKSohUrVmjOnDnq06ePpN+/wnbu3Ll65JFHlJCQoGrVqumbb77R6dOn1axZs2J9qcGV+Pn5acGCBerRo4c6deqkrl27qkmTJrLZbDp48KC++eYbhYWFuT7054k333xTO3fu1N/+9jctXbpUXbt2lTFG+/bt08qVK3X8+HGFhIQUu5bp06drzZo16tixo+rVq6dKlSpp69atWrVqlerXr6++ffuW1svksS5duiglJUUxMTHavn27tm/fnm/NpR/G8/b21uzZsxUXF6eOHTu6fV3wwYMH9fe//73QLxy5WlOnTtW3336rKVOmKCEhQbfffrvOnz+vZcuW6eDBg4qNjdWDDz5YoucALK2s0zaA8qs0z+waY0x6eroZP368ufXWW42/v7+pUqWKufHGG839999vvvjii2LVlpWVZV599VVz++23m5CQEOPj42Nq1qxpBgwYYL7++usiH7tmzRpz9913m9DQUOPr62uioqJM//79zXfffee2zul0mtmzZ5uYmBgTEBBgKleubG688Ubz+OOPu66Pmmf16tWmbdu2xs/Pz4SFhZkHHnjAHD9+vMhLj3lyZjfPoUOHzJNPPmluvPFG4+fnZ4KCgkyjRo3MI488YlatWuW2VsU8s2uMMRkZGWbChAmmYcOGxs/PzwQHB5vmzZubiRMn5rsk2dXWsnz5cvPggw+am2++2QQGBpoqVaqYxo0bm+eeey7fJb0Kc63P7OqSS3sVdivI999/b+666y4TFBRk/P39zW233VbgXzEKc6VLj+3du9cMGTLEREdHGx8fH+Pv72+aNm1qXnzxxXx/eQDgzmaMMdc3XgMAAADXBx9QAwAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZfKlEAZxOp44cOaLAwMArfnc9AAAArj9jjM6cOaOaNWvKy6vw87eE3QIcOXJEtWvXLusyAAAAcAW//faboqKiCp0n7BYgMDBQ0u8vXlBQUBlXAwAAgMtlZmaqdu3artxWGMJuAfK2LgQFBRF2AQAAyrErbTnlA2oAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALMu7rAsAABQsJSVFaWlpCg8PV3R0dFmXAwAVEmEXAMqhlJQUNWzUSOezsuRfubL27N5N4AUAD7CNAQDKobS0NJ3PylKXR8bqfFaW0tLSyrokAKiQCLsAUI5VrRFV1iUAQIVG2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZVbsPutGnTZLPZ9NRTT7nGLly4oJEjRyosLExVqlRR//79dfz4cbfHpaSkqGfPnqpcubIiIiIUHx+vixcvXufqAQAAUB6Uy7C7efNmvfPOO2ratKnb+JgxY7R48WJ99tlnWrdunY4cOaJ+/fq55nNzc9WzZ085HA5t2LBB7733nubNm6eJEyde7xYAAABQDpS7sHv27FkNGjRI7777rqpWreoaz8jI0Jw5c/Tqq6+qa9euatWqlebOnasNGzZo06ZNkqSVK1fqp59+0gcffKDmzZurR48eevHFFzVz5kw5HI6yagkAAABlxLusC7jcyJEj1bNnT3Xr1k1TpkxxjW/ZskU5OTnq1q2ba6xhw4aKjo7Wxo0bFRMTo40bN6pJkyaKjIx0rYmLi9OIESO0a9cutWjRosDnzM7OVnZ2tut+ZmamJOnixYuuLRBeXl7y8vKS0+mU0+l0rc0bz83NlTHmiuN2u102my3f1gq73S7p97PTVzPu7e0tY4zbuM1mk91uz1djYeP0RE/0VH57cjqd8vX1ld1mk6+vr4wx+WqsaD1Z8X2iJ3qip7Lr6Wq3qZarsPvxxx9r69at2rx5c765Y8eOydfXVyEhIW7jkZGROnbsmGvNpUE3bz5vrjBTp07VpEmT8o0nJSUpICBAklStWjU1aNBAycnJOnHihGtNVFSUoqKitG/fPmVkZLjG69evr4iICO3cuVPnz593jTds2FAhISFKSkpye/OaNm0qX19fJSYmutXQunVrORwObd++3TVmt9vVpk0bZWRkaM+ePa5xf39/NWvWTGlpadq/f79rPDg4WI0aNdKRI0d06NAh1zg90RM9ld+e0tPTFR8fr+r1I3RTfLxyc3OVm5tboXuy4vtET/RET2XXU1JSkq6GzVwarcvQb7/9ptatWyshIcG1V7dz585q3ry5ZsyYofnz5+uhhx5yOwMrSbfddpu6dOmi6dOna/jw4Tp48KBWrFjhms/KylJAQICWLl2qHj16FPjcBZ3ZrV27tk6ePKmgoCBJZf/bixV/I6MneqKnwntKTExUu3bt1PuZ6Vo0bZw2bNigli1bVuierPg+0RM90VPZ9XTq1CmFhYUpIyPDldcKUm7O7G7ZskWpqalq2bKlayw3N1fr16/X//7v/2rFihVyOBw6ffq029nd48ePq3r16pKk6tWr64cffnA7bt7VGvLWFMTPz09+fn75xr29veXt7f4S5b1Rl8t74a92/PLjejJus9kKHC+sxuKO0xM9FTZOT9e+Jy8vLzkcDuUaI4fDIZvNVmiNUsXoyYrvEz3REz2Vr54KUm4+oHbHHXdox44d2rZtm+vWunVrDRo0yPXPPj4+WrVqlesxe/fuVUpKimJjYyVJsbGx2rFjh1JTU11rEhISFBQUpMaNG1/3ngAAAFC2ys2Z3cDAQN16661uYwEBAQoLC3ONDxs2TGPHjlVoaKiCgoI0evRoxcbGKiYmRpLUvXt3NW7cWA888IBefvllHTt2TOPHj9fIkSMLPHMLAAAAays3YfdqvPbaa/Ly8lL//v2VnZ2tuLg4vfnmm655u92uJUuWaMSIEYqNjVVAQICGDBmiyZMnl2HVAAAAKCvlOuyuXbvW7X6lSpU0c+ZMzZw5s9DH1KlTR0uXLr3GlQEAAKAiKDd7dgEAAIDSRtgFAACAZRF2AQAAYFmEXQAAAFgWYRcAAACWRdgFAACAZRF2AQAAYFmEXQAAAFgWYRcAAACWRdgFAACAZRF2AQAAYFmEXQAAAFgWYRcAAACWRdgFAACAZRF2AQAAYFmEXQAAAFgWYRcAAACWRdgFAACAZRF2AQAAYFmEXQAAAFgWYRcAAACWRdgFAACAZRF2AQAAYFmEXQAAAFiWd1kXAFhBSkqK0tLSFB4erujo6LIuBwAA/H+EXaCEUlJS1LBRI53PypJ/5cras3s3gRcAgHKCbQxACaWlpel8Vpa6PDJW57OylJaWVtYlAQCA/4+wC5SSqjWiyroEAABwGcIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLI/D7rZt2/TRRx+5ja1YsUIdO3ZU27Zt9frrr5e4OAAAAKAkPA67//M//6NPPvnEdT85OVl9+/ZVcnKyJGns2LGaNWtWySsEAAAAPORx2P3xxx/Vvn171/33339fdrtdSUlJ+v777zVgwAC9/fbbpVIkAAAA4AmPw25GRobCwsJc95cuXao777xT4eHhkqQ777xTv/zyS7GO+dZbb6lp06YKCgpSUFCQYmNjtWzZMtf8hQsXNHLkSIWFhalKlSrq37+/jh8/7naMlJQU9ezZU5UrV1ZERITi4+N18eJFT9sEAABABeZx2K1Ro4Z2794tSTp69Ki2bNmi7t27u+bPnj0rL6/iHT4qKkrTpk3Tli1blJiYqK5du6p3797atWuXJGnMmDFavHixPvvsM61bt05HjhxRv379XI/Pzc1Vz5495XA4tGHDBr333nuaN2+eJk6c6GmbAAAAqMC8PX1g79699cYbb+jChQv6/vvv5efnp759+7rmf/zxR9WvX79Yx+zVq5fb/b/97W966623tGnTJkVFRWnOnDmaP3++unbtKkmaO3euGjVqpE2bNikmJkYrV67UTz/9pK+//lqRkZFq3ry5XnzxRY0bN04vvPCCfH19PW0XAAAAFZDHYXfKlCk6ceKE/v3vfyskJETz5s1TZGSkJCkzM1MLFizQyJEjPS4sNzdXn332mc6dO6fY2Fht2bJFOTk56tatm2tNw4YNFR0drY0bNyomJkYbN25UkyZNXHVIUlxcnEaMGKFdu3apRYsWBT5Xdna2srOzXfczMzMlSRcvXnRtgfDy8pKXl5ecTqecTqdrbd54bm6ujDFXHLfb7bLZbPm2VtjtdlffVzPu7e0tY4zbuM1mk91uz1djYeP0VDo9Xfo8vr6+cjqdrsdV1J6KGqen69OT0+mUr6+v7DabfH19ZYzJV2NF68mK7xM90RM9lV1PV7tN1eOwW6VKFX344YeFzh06dEiVK1cu9nF37Nih2NhYXbhwQVWqVNHChQvVuHFjbdu2Tb6+vgoJCXFbHxkZqWPHjkmSjh075hZ08+bz5gozdepUTZo0Kd94UlKSAgICJEnVqlVTgwYNlJycrBMnTrjWREVFKSoqSvv27VNGRoZrvH79+oqIiNDOnTt1/vx513jDhg0VEhKipKQktzevadOm8vX1VWJiolsNrVu3lsPh0Pbt211jdrtdbdq0UUZGhvbs2eMa9/f3V7NmzZSWlqb9+/e7xoODg9WoUSMdOXJEhw4dco3TU+n0lJGRIV9fXwX42BUfH6+TJ08qMTGxQvdkxfepovWUnp6u+Ph4Va8foZvi45Wbm6vc3NwK3ZMV3yd6oid6KruekpKSdDVs5tJoXQIZGRmqUqWKK217yuFwKCUlRRkZGVqwYIFmz56tdevWadu2bXrooYfczsBK0m233aYuXbpo+vTpGj58uA4ePKgVK1a45rOyshQQEKClS5eqR48eBT5nQWd2a9eurZMnTyooKEhS2f/2YsXfyKzS07Zt29SmTRv1m/Cqlkx/Rt99952aN29eoXsqapyerk9PiYmJateunXo/M12Lpo3Thg0b1LJlywrdkxXfJ3qiJ3oqu55OnTqlsLAwZWRkuPJaQTw+sytJiYmJGj9+vNavXy+Hw6GVK1eqa9euSktL07BhwzRmzBh17ty5WMf09fXVDTfcIElq1aqVNm/erNdff1333nuvHA6HTp8+7XZ29/jx46pevbokqXr16vrhhx/cjpd3tYa8NQXx8/OTn59fvnFvb295e7u/RHlv1OUKC/mFjV9+XE/GbTZbgeOF1VjccXq6utovfR6HwyEvLy+3+YrY05XG6ena9+Tl5SWHw6FcY+RwOGSz2QqtUaoYPVnxfaIneqKn8tVTQTy+GsOGDRvUvn17/fzzzxo8eLBbog8PD1dGRobeeecdTw/v4nQ6lZ2drVatWsnHx0erVq1yze3du1cpKSmKjY2VJMXGxmrHjh1KTU11rUlISFBQUJAaN25c4loAAABQsXh8Zve5555zXQnhzJkzmj17ttt8ly5d9N577xXrmM8++6x69Oih6OhonTlzRvPnz9fatWu1YsUKBQcHa9iwYRo7dqxCQ0MVFBSk0aNHKzY2VjExMZKk7t27q3HjxnrggQf08ssv69ixYxo/frxGjhxZ4JlbAAAAWJvHYXfz5s2aOnWq/Pz8dPbs2XzztWrVKvJDYQVJTU3Vgw8+qKNHjyo4OFhNmzbVihUrdOedd0qSXnvtNXl5eal///7Kzs5WXFyc3nzzTdfj7Xa7lixZohEjRig2NlYBAQEaMmSIJk+e7GmbAAAAqMA8Drs+Pj5uWxcud/jwYVWpUqVYx5wzZ06R85UqVdLMmTM1c+bMQtfUqVNHS5cuLdbzAgAAwJo83rMbExOjBQsWFDh37tw5zZ07V506dfK4MAAAAKCkPA67kyZNUmJionr27Klly5ZJ+v1b02bPnq1WrVrpxIkTmjBhQqkVCgAAABSXx9sY2rZtq6VLl2rEiBF68MEHJUlPP/20JKlBgwZaunSpmjZtWjpVAgAAAB4o0XV2u3btqr1792rbtm36+eef5XQ61aBBA7Vq1Uo2m620agQAAAA8UqKwm6d58+aub4wCAAAAyguP9+x+9NFHGjp0aKHzDz30kD799FNPDw8AAACUmMdh97XXXivyixr8/f312muveXp4AAAAoMQ8Drt79+5VixYtCp1v1qyZ9uzZ4+nhAQAAgBLzOOwaY3T69OlC50+dOqWcnBxPDw8AAACUmMdht0WLFvroo4/kcDjyzWVnZ2v+/PlFnvkFAAAArjWPw+4zzzyjnTt3qkuXLlq8eLH279+v/fv366uvvlLnzp21a9cuPfPMM6VZKwAAAFAsHl96rEePHpozZ46efPJJ9enTxzVujFFgYKDeffdd9ezZszRqBAAAADxSouvsDh06VP369VNCQoJ+/fVXSb9/e1r37t0VGBhYKgUCAAAAnirxl0oEBQWpf//+pVELAAAAUKpKHHbPnDmjgwcP6tSpUzLG5Jvv2LFjSZ8CAAAA8IjHYffkyZMaNWqUPv/8c+Xm5kr6fb+uzWZz++e8OQAAAOB68zjsPvroo1q8eLGeeOIJdejQQVWrVi3NugAAAIAS8zjsrly5UmPGjNHLL79cmvUAAAAApcbj6+xWrlxZdevWLcVSAAAAgNLlcdgdPHiwFi5cWJq1AAAAAKXK420MAwYM0Lp163TXXXdp+PDhql27tux2e751LVu2LFGBAAAAgKc8Drvt27d3/XNCQkK+ea7GAAAAgLLmcdidO3duadYBAAAAlDqPw+6QIUNKsw4AAACg1Hn8AbVLHT16VD/++KPOnTtXGocDAAAASkWJwu6iRYvUsGFDRUVFqWXLlvr+++8lSWlpaWrRogVXawAAAECZ8jjsLl68WP369VN4eLief/55GWNcc+Hh4apVq5bmzZtXGjUCAAAAHvE47E6ePFkdO3bUt99+q5EjR+abj42NVVJSUomKAwAAAErC47C7c+dODRw4sND5yMhIpaamenp4AAAAoMRK9HXBRX0gbf/+/QoLC/P08AAAAECJeRx2u3Tpovfee08XL17MN3fs2DG9++676t69e4mKAwAAAErC47A7ZcoUHTp0SG3atNE777wjm82mFStWaPz48WrSpImMMXr++edLs1YAAACgWDwOuw0bNtR3332nsLAwTZgwQcYYvfLKK3rppZfUpEkTffPNN6pbt24plgoAAAAUj0ffoJaTk6Pdu3crNDRUX3/9tU6dOqVffvlFTqdT9evXV7Vq1Uq7TgAAAKDYPDqz6+XlpVatWumLL76QJFWtWlVt2rRR27ZtCboAAAAoNzwKu3a7XXXq1FF2dnZp1wMAAACUGo/37I4ePVqzZs1Senp6adYDAAAAlBqP9uxKUm5urvz8/NSgQQMNGDBAdevWlb+/v9sam82mMWPGlLhIAAAAwBMeh92//OUvrn+eM2dOgWsIuwAAAChLHofd5OTk0qwDAAAAKHUehd3z58/r9ddfV5cuXdSrV6/SrgkAAAAoFR59QM3f31/vvPOOjh8/Xtr1AAAAAKXG46sxtGrVSjt37izNWgAAAIBS5XHYnTFjhj7++GPNnj1bFy9eLM2aAAAAgFLh8QfUhg4dKi8vLz322GN64oknVKtWrQIvPfbjjz+WuEgAAADAEx6H3dDQUIWFhenmm28uzXoAAACAUuNx2F27dm0plgEAAACUPo/37AIAAADlncdndtevX39V6zp27OjpUwAAAAAl4nHY7dy5s2w22xXX5ebmevoUAAAAQIl4HHbXrFmTbyw3N1cHDhzQrFmz5HQ6NW3atBIVBwAAAJSEx2G3U6dOhc4NHTpUHTp00Nq1a9W1a1dPnwIAAAAokWvyATUvLy/dd999mj179rU4PAAAAHBVrtnVGNLT03X69OlrdXgAAADgijzexpCSklLg+OnTp7V+/Xq98sor6tChg8eFAQAAACXlcditW7duoVdjMMYoJiZG77zzjseFAQAAACXlcdj917/+lS/s2mw2Va1aVQ0aNFDjxo1LXBwAAABQEh6H3aFDh5ZiGQAAAEDp8/gDaunp6dq+fXuh8zt27NCpU6c8PTwAAABQYh6H3TFjxmj48OGFzj/22GP6y1/+4unhAQAAgBLzOOyuXr1a99xzT6HzvXr10tdff+3p4QEAAIAS8zjsnjhxQuHh4YXOh4WFKTU11dPDAwAAACXmcditUaOGkpKSCp3fsmWLqlWr5unhAQAAgBLzOOz26dNHc+bM0VdffZVvbtGiRZo7d6769u1bouIAAACAkvD40mMvvPCCvv76a/Xt21fNmjXTrbfeKknauXOnfvzxRzVq1EiTJk0qtUIBAACA4vL4zG5wcLA2bdqk8ePHKycnRwsWLNCCBQuUk5OjCRMm6Pvvv1dISEgplgoAAAAUj8dndiUpICBAkyZN4gwuAAAAyiWPz+xevHhRmZmZhc5nZmbq4sWLxTrm1KlT1aZNGwUGBioiIkJ9+vTR3r173dZcuHBBI0eOVFhYmKpUqaL+/fvr+PHjbmtSUlLUs2dPVa5cWREREYqPjy92LQAAAKj4PA67TzzxhG6//fZC59u1a6enn366WMdct26dRo4cqU2bNikhIUE5OTnq3r27zp0751ozZswYLV68WJ999pnWrVunI0eOqF+/fq753Nxc9ezZUw6HQxs2bNB7772nefPmaeLEicVvEgAAABWax2F3+fLlGjBgQKHzAwYM0NKlS4t9zKFDh+qWW25Rs2bNNG/ePKWkpGjLli2SpIyMDM2ZM0evvvqqunbtqlatWmnu3LnasGGDNm3aJElauXKlfvrpJ33wwQdq3ry5evTooRdffFEzZ86Uw+HwtF0AAABUQB7v2T1y5Ihq1apV6HzNmjV1+PBhTw8v6fdwK0mhoaGSfr92b05Ojrp16+Za07BhQ0VHR2vjxo2KiYnRxo0b1aRJE0VGRrrWxMXFacSIEdq1a5datGiR73mys7OVnZ3tup+3PePixYuu7Q9eXl7y8vKS0+mU0+l0rc0bz83NlTHmiuN2u102my3ftgq73S7p9zPTVzPu7e0tY4zbuM1mk91uz1djYeP0VDo9Xfo8vr6+cjqdrsdV1J6KGqen69OT0+mUr6+v7DabfH19ZYzJV2NF68mK7xM90RM9lV1PV7tF1eOwGxYWlm8/7aV2796toKAgTw8vp9Opp556Su3atXNd1uzYsWPy9fXNd5WHyMhIHTt2zLXm0qCbN583V5CpU6cW+CG7pKQkBQQESJKqVaumBg0aKDk5WSdOnHCtiYqKUlRUlPbt2+cK55JUv359RUREaOfOnTp//rxrvGHDhgoJCVFSUpLbm9e0aVP5+voqMTHRrYbWrVvL4XBo+/btrjG73a42bdooIyNDe/bscY37+/urWbNmSktL0/79+13jwcHBatSokY4cOaJDhw65xumpdHrKyMiQr6+vAnzsio+P18mTJ5WYmFihe7Li+1TRekpPT1d8fLyq14/QTfHxys3NVW5uboXuyYrvEz3REz2VXU9FfbnZpWzm0mhdDMOGDdOnn36q9evX5ztbunXrVnXs2FF//OMfNXfuXE8OrxEjRmjZsmX69ttvFRUVJUmaP3++HnroIbezsJJ02223qUuXLpo+fbqGDx+ugwcPasWKFa75rKwsBQQEaOnSperRo0e+5yrozG7t2rV18uRJV2Av699erPgbmVV62rZtm9q0aaN+E17VkunP6LvvvlPz5s0rdE9FjdPT9ekpMTFR7dq1U+9npmvRtHHasGGDWrZsWaF7suL7RE/0RE9l19OpU6cUFhamjIyMIk+wenxm98UXX9Ty5ct12223qVevXm5fKrF48WJFREToxRdf9OjYo0aN0pIlS7R+/XpX0JWk6tWry+Fw6PTp025nd48fP67q1au71vzwww9ux8u7WkPemsv5+fnJz88v37i3t7e8vd1forw36nJ5L/zVjl9+XE/GbTZbgeOF1VjccXq6utovfR6HwyEvLy+3+YrY05XG6ena9+Tl5SWHw6FcY+RwOGSz2QqtUaoYPVnxfaIneqKn8tVTQTz+gFrNmjW1efNm3X///Vq9erWmTJmiKVOmaPXq1Ro0aJA2b97sFlSvhjFGo0aN0sKFC7V69WrVq1fPbb5Vq1by8fHRqlWrXGN79+5VSkqKYmNjJUmxsbHasWOHUlNTXWsSEhIUFBSkxo0be9ouAAAAKqASfalEzZo19eabb2rq1Km6ePGiKlWqpGrVqslms3l0vJEjR2r+/PlatGiRAgMDXXtsg4OD5e/vr+DgYA0bNkxjx45VaGiogoKCNHr0aMXGxiomJkaS1L17dzVu3FgPPPCAXn75ZR07dkzjx4/XyJEjCzx7CwAAAOvy6MzugQMH9Oc//1l16tRRUFCQateurXr16ql169YaNWqUDhw44FExb731ljIyMtS5c2fVqFHDdfvkk09ca1577TXdfffd6t+/vzp27Kjq1avriy++cM3b7XYtWbJEdrtdsbGxGjx4sB588EFNnjzZo5oAAABQcRX7zO6iRYv0wAMP6OzZs6pbt6569eqlwMBAnTlzRtu3b9dbb72l999/Xx988IF69+5drGNfzWflKlWqpJkzZ2rmzJmFrqlTp06xr/ELAAAA6ylW2P3pp5907733qn79+nrnnXfUoUOHfGu++eYbPf7447rvvvu0ZcsW9skCAACgzBRrG8NLL72k8PBwffvttwUGXUnq0KGDvvnmG4WFhWnq1KmlUiQAAADgiWKF3TVr1mjYsGGubzQrTGhoqB5++GGtXr26RMUBAAAAJVGssHvy5EnVrVv3qtbWq1dPJ0+e9KQmAAAAoFQUK+yGh4crOTn5qtYmJycrPDzco6IAAACA0lCssNu5c2fNmTNH6enpRa5LT0/XnDlz1Llz55LUBgAAAJRIscLuc889p5MnT6pjx47asGFDgWs2bNigTp066eTJk3r22WdLpUgAAADAE8W69Fjjxo01f/58Pfjgg+rQoYPq1q2rZs2auV1nNzk5WZUqVdIHH3ygW2655VrVDQAAAFxRsb9Uol+/fmrevLlefvllLVmyRF9++aVrrkaNGnrkkUcUHx+vG264oTTrBAAAAIqt2GFXkurXr6+3335bkpSZmakzZ84oMDBQQUFBpVocAAAAUBIehd1LBQUFEXIBAABQLhXrA2oAAABARULYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAQImkpKQoJSWlrMsoEGEXAAAAHktJSVHDRo3UsFGjchl4CbsAAADwWFpams5nZel8VpbS0tLKupx8ylXYXb9+vXr16qWaNWvKZrPpyy+/dJs3xmjixImqUaOG/P391a1bN/38889ua9LT0zVo0CAFBQUpJCREw4YN09mzZ69jFwAAACgvylXYPXfunJo1a6aZM2cWOP/yyy/rn//8p95++219//33CggIUFxcnC5cuOBaM2jQIO3atUsJCQlasmSJ1q9fr+HDh1+vFgAAAFCOeJd1AZfq0aOHevToUeCcMUYzZszQ+PHj1bt3b0nS+++/r8jISH355Ze67777tHv3bi1fvlybN29W69atJUlvvPGG/vCHP+jvf/+7atased16AQAAQNkrV2G3KMnJyTp27Ji6devmGgsODlbbtm21ceNG3Xfffdq4caNCQkJcQVeSunXrJi8vL33//ffq27dvgcfOzs5Wdna2635mZqYk6eLFi7p48aIkycvLS15eXnI6nXI6na61eeO5ubkyxlxx3G63y2azuY576bgk5ebmXtW4t7e3jDFu4zabTXa7PV+NhY3TU+n0dOnz+Pr6yul0uh5XUXsqapyerk9PTqdTvr6+stts8vX1lTEmX40VrScrvk/0RE/05CVjjHx9fSXJteZ69HT5+sJUmLB77NgxSVJkZKTbeGRkpGvu2LFjioiIcJv39vZWaGioa01Bpk6dqkmTJuUbT0pKUkBAgCSpWrVqatCggZKTk3XixAnXmqioKEVFRWnfvn3KyMhwjdevX18RERHauXOnzp8/7xpv2LChQkJClJSU5PbmNW3aVL6+vkpMTHSroXXr1nI4HNq+fbtrzG63q02bNsrIyNCePXtc4/7+/mrWrJnS0tK0f/9+13hwcLAaNWqkI0eO6NChQ65xeiqdnjIyMuTr66sAH7vi4+N18uRJJSYmVuierPg+VbSe0tPTFR8fr+r1I3RTfLxyc3OVm5tboXuy4vtET/RETw109uxZxcfHS5JOnjypI0eOXJeekpKSdDVs5tJoXY7YbDYtXLhQffr0kSRt2LBB7dq105EjR1SjRg3XuoEDB8pms+mTTz7RSy+9pPfee0979+51O1ZERIQmTZqkESNGFPhcBZ3ZrV27tk6ePKmgoCBJ1v6NjJ5K1tO2bdvUpk0b9ZvwqpZMf0bfffedmjdvXqF7Kmqcnq5PT4mJiWrXrp16PzNdi6aN04YNG9SyZcsK3ZMV3yd6oid68tKWLVt0++23S5K+++47tWzZ8rr0dOrUKYWFhSkjI8OV1wpSYc7sVq9eXZJ0/Phxt7B7/PhxV7CoXr26UlNT3R538eJFpaenux5fED8/P/n5+eUb9/b2lre3+0uU90ZdLu+Fv9rxy4/rybjNZitwvLAaiztOT1dX+6XP43A45OXl5TZfEXu60jg9XfuevLy85HA4lGuMHA6HbDZboTVKFaMnK75P9ERP9PT7cRwOR741ZdFTQcrV1RiKUq9ePVWvXl2rVq1yjWVmZur7779XbGysJCk2NlanT5/Wli1bXGtWr14tp9Optm3bXveaAQAAULbK1Znds2fP6pdffnHdT05O1rZt2xQaGqro6Gg99dRTmjJlim688UbVq1dPEyZMUM2aNV1bHRo1aqS77rpLjz76qN5++23l5ORo1KhRuu+++7gSAwAAwH+hchV2ExMT1aVLF9f9sWPHSpKGDBmiefPm6X/+53907tw5DR8+XKdPn1b79u21fPlyVapUyfWYDz/8UKNGjdIdd9whLy8v9e/fX//85z+vey8AAAAoe+Uq7Hbu3Nltw/LlbDabJk+erMmTJxe6JjQ0VPPnz78W5QEAAKCCqTB7dgEAAIDiIuwCAADAsgi7AAAAsCzCLgAAACyLsAsAAADLIuwCAADAsgi7AAAAsCzCLgAAACyLsAsAAADLIuwCAADAsgi7AAAAsCzCLgAAACyLsAsAAADLIuwCAADAsgi7AAAAsCzCLgAAACyLsAsAAADLIuwCAADAsgi7AAAAsCzCLgAAACyLsAsAAADLIuwCAADAsrzLugCUXykpKUpLS1N4eLiio6PLuhwAAIBiI+yiQCkpKWrYqJHOZ2XJv3Jl7dm9m8ALAAAqHLYxoEBpaWk6n5WlLo+M1fmsLKWlpZV1SQAAAMVG2EWRqtaIKusSAAAAPEbYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGURdgEAAGBZhF0AAABYFmEXAAAAlkXYBQAAgGVZNuzOnDlTdevWVaVKldS2bVv98MMPZV0SAAAArjNLht1PPvlEY8eO1fPPP6+tW7eqWbNmiouLU2pqalmXBgAAgOvIkmH31Vdf1aOPPqqHHnpIjRs31ttvv63KlSvrX//6V1mXVqiUlBRt3bpVKSkpZV0KAACAZXiXdQGlzeFwaMuWLXr22WddY15eXurWrZs2btxY4GOys7OVnZ3tup+RkSFJSk9P18WLF13H8PLyktPplNPpdDu2l5eXcnNzZYy54rjdbpfNZnMdV5IOHz6stjExOp+VpaDgYH37zTeqVauWa70k5ebmutXs7e0tY4zbuM1mk91uz1djYeNF9XT27Fl5e3vr1G/J8vHxUWZmpk6fPn3VPRVVe1n1VNL3qbDaMzMzJUknDv7qeq3S09MrdE9FjdPT9ekpMzNTPj4+rv8PnjlzRhkZGRW6Jyu+T/RET/TkpTNnzsjHx0eSiswLpd3TqVOnJMntWAUyFnP48GEjyWzYsMFtPD4+3tx2220FPub55583krhx48aNGzdu3LhVsNtvv/1WZDa03JldTzz77LMaO3as677T6VR6errCwsJks9muSw2ZmZmqXbu2fvvtNwUFBV2X57yS8lhTecVrhWuBf68AVBRl8fPKGKMzZ86oZs2aRa6zXNgNDw+X3W7X8ePH3caPHz+u6tWrF/gYPz8/+fn5uY2FhIRcqxKLFBQUVO7+o1YeayqveK1wLfDvFYCK4nr/vAoODr7iGst9QM3X11etWrXSqlWrXGNOp1OrVq1SbGxsGVYGAACA681yZ3YlaezYsRoyZIhat26t2267TTNmzNC5c+f00EMPlXVpAAAAuI4sGXbvvfdenThxQhMnTtSxY8fUvHlzLV++XJGRkWVdWqH8/Pz0/PPP59tOUZbKY03lFa8VrgX+vQJQUZTnn1c2Y650vQYAAACgYrLcnl0AAAAgD2EXAAAAlkXYBQAAgGURdgEAAGBZhN3raP369erVq5dq1qwpm82mL7/8Mt+a3bt365577lFwcLACAgLUpk0bpaSkXNO63nrrLTVt2tR1IejY2FgtW7ZMkpSenq7Ro0fr5ptvlr+/v6Kjo/XEE08oIyPjmtZUXh0+fFiDBw9WWFiY/P391aRJEyUmJha49vHHH5fNZtOMGTOub5Eo14r6OZCTk6Nx48apSZMmCggIUM2aNfXggw/qyJEjbsfYt2+fevfurfDwcAUFBal9+/Zas2bNde4EgJVNnTpVbdq0UWBgoCIiItSnTx/t3bvXbU3nzp1ls9ncbo8//ni+Y82bN09NmzZVpUqVFBERoZEjR16vNiQRdq+rc+fOqVmzZpo5c2aB87/++qvat2+vhg0bau3atdq+fbsmTJigSpUqXdO6oqKiNG3aNG3ZskWJiYnq2rWrevfurV27dunIkSM6cuSI/v73v2vnzp2aN2+eli9frmHDhl3TmsqjU6dOqV27dvLx8dGyZcv0008/6R//+IeqVq2ab+3ChQu1adOmK36FIf77FPVzICsrS1u3btWECRO0detWffHFF9q7d6/uuecet3V33323Ll68qNWrV2vLli1q1qyZ7r77bh07dux6tQHA4tatW6eRI0dq06ZNSkhIUE5Ojrp3765z5865rXv00Ud19OhR1+3ll192m3/11Vf117/+Vc8884x27dqlr7/+WnFxcdezFcmgTEgyCxcudBu79957zeDBg8umoMtUrVrVzJ49u8C5Tz/91Pj6+pqcnJzrXFXZGjdunGnfvv0V1x06dMjUqlXL7Ny509SpU8e89tpr1744VEgF/Ry43A8//GAkmYMHDxpjjDlx4oSRZNavX+9ak5mZaSSZhISEa1kugP9iqampRpJZt26da6xTp07mySefLPQx6enpxt/f33z99dfXocLCcWa3nHA6nfrPf/6jm266SXFxcYqIiFDbtm0L3OpwLeXm5urjjz/WuXPnCv165YyMDAUFBcnb25LfSVKor776Sq1bt9Yf//hHRUREqEWLFnr33Xfd1jidTj3wwAOKj4/XLbfcUkaVwkoyMjJks9kUEhIiSQoLC9PNN9+s999/X+fOndPFixf1zjvvKCIiQq1atSrbYgFYVt72xdDQULfxDz/8UOHh4br11lv17LPPKisryzWXkJAgp9Opw4cPq1GjRoqKitLAgQP122+/XdfaCbvlRGpqqs6ePatp06bprrvu0sqVK9W3b1/169dP69atu+bPv2PHDlWpUkV+fn56/PHHtXDhQjVu3DjfurS0NL344osaPnz4Na+pvNm/f7/eeust3XjjjVqxYoVGjBihJ554Qu+9955rzfTp0+Xt7a0nnniiDCuFVVy4cEHjxo3Tn/70JwUFBUmSbDabvv76ayUlJSkwMFCVKlXSq6++quXLlxe4pQYASsrpdOqpp55Su3btdOutt7rG77//fn3wwQdas2aNnn32Wf373//W4MGDXfP79++X0+nUSy+9pBkzZmjBggVKT0/XnXfeKYfDcf0aKNPzyv/FdNmfLw8fPmwkmT/96U9u63r16mXuu+++a15Pdna2+fnnn01iYqJ55plnTHh4uNm1a5fbmoyMDHPbbbeZu+66yzgcjmteU3nj4+NjYmNj3cZGjx5tYmJijDHGJCYmmsjISHP48GHXPNsYUJTLfw5cyuFwmF69epkWLVqYjIwM17jT6TT33HOP6dGjh/n222/Nli1bzIgRI0ytWrXMkSNHrlPlAP6bPP7446ZOnTrmt99+K3LdqlWrjCTzyy+/GGOM+dvf/mYkmRUrVrjWpKamGi8vL7N8+fJrWvOlOLNbToSHh8vb2zvf2dRGjRpd86sxSJKvr69uuOEGtWrVSlOnTlWzZs30+uuvu+bPnDmju+66S4GBgVq4cKF8fHyueU3lTY0aNYp8f7755hulpqYqOjpa3t7e8vb21sGDB/X000+rbt26ZVAxKqqcnBwNHDhQBw8eVEJCguusriStXr1aS5Ys0ccff6x27dqpZcuWevPNN+Xv7+/2VwYAKA2jRo3SkiVLtGbNGkVFRRW5tm3btpKkX375RdLv/92U5PbfzmrVqik8PPy6ZJs8/12bLssxX19ftWnTJt9lPfbt26c6depc93qcTqeys7MlSZmZmYqLi5Ofn5+++uqra351iPKqXbt2Rb4/DzzwgLp16+Y2HxcXpwceeEAPPfTQdasTFVte0P3555+1Zs0ahYWFuc3n7Yfz8nI/V+Hl5SWn03nd6gRgbcYYjR49WgsXLtTatWtVr169Kz5m27Ztkv4v5LZr106StHfvXldQTk9PV1pa2nXNNoTd6+js2bOu33YkKTk5Wdu2bVNoaKiio6MVHx+ve++9Vx07dlSXLl20fPlyLV68WGvXrr2mdT377LPq0aOHoqOjdebMGc2fP19r167VihUrlJmZqe7duysrK0sffPCBMjMzlZmZKen3387sdvs1ra08GTNmjG6//Xa99NJLGjhwoH744QfNmjVLs2bNkvT7B4cuDyY+Pj6qXr26br755rIoGeVQUT8HatSooQEDBmjr1q1asmSJcnNzXZcTCw0Nla+vr2JjY1W1alUNGTJEEydOlL+/v959910lJyerZ8+eZdUWAIsZOXKk5s+fr0WLFikwMND1syg4OFj+/v769ddfNX/+fP3hD39QWFiYtm/frjFjxqhjx45q2rSpJOmmm25S79699eSTT2rWrFkKCgrSs88+q4YNG6pLly7Xr5nrtmECZs2aNUZSvtuQIUNca+bMmWNuuOEGU6lSJdOsWTPz5ZdfXvO6Hn74YVOnTh3j6+trqlWrZu644w6zcuXKImuWZJKTk695beXN4sWLza233mr8/PxMw4YNzaxZs4pcz55dXK6onwPJycmF/v9tzZo1rmNs3rzZdO/e3YSGhprAwEATExNjli5dWnZNAbCcwn4WzZ071xhjTEpKiunYsaMJDQ01fn5+5oYbbjDx8fFunzEw5vfP+zz88MMmJCTEhIaGmr59+5qUlJTr2ovt/zcEAAAAWA4fUAMAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AUAAIBlEXYBAABgWYRdAAAAWBZhFwAAAJZF2AWAMjB06FDVrVvXbcxms+mFF14ok3quRkE1A0B5R9gFgFJis9mu6rZ27dprWsfWrVtls9k0fvz4Qtf8/PPPstlsGjt27DWtBQDKmndZFwAAVvHvf//b7f7777+vhISEfOONGjXSu+++K6fTeU3qaNmypRo2bKiPPvpIU6ZMKXDN/PnzJUmDBw++JjUAQHlB2AWAUnJ5cNy0aZMSEhLKJFAOGjRIEyZM0KZNmxQTE5Nv/qOPPlLDhg3VsmXL614bAFxPbGMAgDJwtftfDx8+rIcffliRkZHy8/PTLbfcon/9619XfNygQYMk/d8Z3Ett2bJFe/fuda1ZtGiRevbsqZo1a8rPz08NGjTQiy++qNzc3CKfY+3atQVuyzhw4IBsNpvmzZvnNr5nzx4NGDBAoaGhqlSpklq3bq2vvvrqir0AQElwZhcAyqnjx48rJiZGNptNo0aNUrVq1bRs2TINGzZMmZmZeuqppwp9bL169XT77bfr008/1WuvvSa73e6aywvA999/vyRp3rx5qlKlisaOHasqVapo9erVmjhxojIzM/XKK6+USi+7du1Su3btVKtWLT3zzDMKCAjQp59+qj59+ujzzz9X3759S+V5AOByhF0AKKf++te/Kjc3Vzt27FBYWJgk6fHHH9ef/vQnvfDCC3rsscfk7+9f6OMHDRqkkSNHatWqVerevbskyel06pNPPlFsbKzq168v6ffwe+lxHn/8cT3++ON68803NWXKFPn5+ZW4lyeffFLR0dHavHmz63h//vOf1b59e40bN46wC+CaYRsDAJRDxhh9/vnn6tWrl4wxSktLc93i4uKUkZGhrVu3FnmMe++9Vz4+Pm5bGdatW6fDhw+7tjBIcgu6Z86cUVpamjp06KCsrCzt2bOnxL2kp6dr9erVGjhwoOv4aWlpOnnypOLi4vTzzz/r8OHDJX4eACgIZ3YBoBw6ceKETp8+rVmzZmnWrFkFrklNTS3yGGFhYYqLi9PChQv19ttvq1KlSpo/f768vb01cOBA17pdu3Zp/PjxWr16tTIzM92OkZGRUeJefvnlFxljNGHCBE2YMKHQXmrVqlXi5wKAyxF2AaAcyrss2eDBgzVkyJAC1zRt2vSKxxk8eLCWLFmiJUuW6J577tHnn3+u7t27q1q1apKk06dPq1OnTgoKCtLkyZPVoEEDVapUSVu3btW4ceOKvDyazWYrcPzyD7blHeMvf/mL4uLiCnzMDTfccMVeAMAThF0AKIeqVaumwMBA5ebmqlu3bh4f55577lFgYKDmz58vHx8fnTp1ym0Lw9q1a3Xy5El98cUX6tixo2s8OTn5iseuWrWqpN8D86UOHjzodj9vb7CPj0+JegEAT7BnFwDKIbvdrv79++vzzz/Xzp07882fOHHiqo7j7++vvn37aunSpXrrrbcUEBCg3r17uz2P9Pse4TwOh0NvvvnmFY9dp04d2e12rV+/3m388sdGRESoc+fOeuedd3T06FGPewEAT3BmFwDKqWnTpmnNmjVq27atHn30UTVu3Fjp6enaunWrvv76a6Wnp1/VcQYPHqz3339fK1as0KBBgxQQEOCau/3221W1alUNGTJETzzxhGw2m/7973+7hd/CBAcH649//KPeeOMN2Ww2NWjQQEuWLClwL/HMmTPVvn17NWnSRI8++qjq16+v48ePa+PGjTp06JB+/PHHq39hAKAYCLsAUE5FRkbqhx9+0OTJk/XFF1/ozTffVFhYmG655RZNnz79qo/TtWtX1ahRQ0ePHnXbwiD9/iG2JUuW6Omnn9b48eNVtWpVDR48WHfccUeh+2sv9cYbbygnJ0dvv/22/Pz8NHDgQL3yyiu69dZb3dY1btxYiYmJmjRpkubNm6eTJ08qIiJCLVq00MSJE6+6FwAoLpu5ml/fAQAAgAqIPbsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCyCLsAAACwLMIuAAAALIuwCwAAAMsi7AIAAMCy/h80yiBaXK+FfgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "975dabe86fdea077"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
